{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the dataframe is 41932\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"CH22_Demand_XY_Train.csv\")\n",
    "print(f\"the length of the dataframe is {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                   DateTime        X1     X2      X3      X4             Y\n",
       "0      2022-01-01 00:00:00  2.186333  13.76  0.0663  0.1547  521163.83540\n",
       "1      2022-01-01 00:10:00  2.138000  13.90  0.0910  0.1105  449066.62018\n",
       "2      2022-01-01 00:20:00  2.104333  13.90  0.0806  0.1300  437394.72159\n",
       "3      2022-01-01 00:30:00  2.040333  14.00  0.1183  0.1248  422107.63292\n",
       "4      2022-01-01 00:40:00  1.973667  14.14  0.0624  0.1105  406923.83540\n",
       "...                    ...       ...    ...     ...     ...           ...\n",
       "41927  2022-10-19 03:50:00  5.856667  17.66  0.1092  0.1391  365929.91028\n",
       "41928  2022-10-19 04:00:00  5.860000  17.66  0.1183  0.1495  368822.51417\n",
       "41929  2022-10-19 04:10:00  5.846667  17.68  0.1001  0.1976  373857.78769\n",
       "41930  2022-10-19 04:20:00  5.856667  17.66  0.1183  0.1391  373536.38739\n",
       "41931  2022-10-19 04:30:00  5.876667  17.68  0.0767  0.1690  376643.25826\n",
       "\n",
       "[41932 rows x 6 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps:\n",
    "# 1. use the train_data set to do feature engineer and get f1, f2, f3, f4\n",
    "# 2. normalize the f1, f2, f3, f4 by mean_val, log_scale the y (save the mean_val)\n",
    "# 3. split the train_data to train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1: the same time of last 7 days (extract from y);  shape=(7*1,)\n",
    "# f2: the datetime as a categorical value (hh:mm); shape=(1,)\n",
    "# f3: the previous two hour x1, x2, x3, x4 multi-value time series;  shape=(2*6,5)\n",
    "# f4: the previous two hour y; shape=(2*6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed f1, f3, f4 to two different GRU, get o1, o2, o3\n",
    "# feed f2, o1, o2, o3 to MLP and get final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[['X1','X2','X3','X4']],df['Y']\n",
    "time_point_week=7*24*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the f1\n",
    "f1=[]\n",
    "for i in range(time_point_week,len(df)):\n",
    "    last_week_demand = []\n",
    "    for j in range(7):\n",
    "        last_week_demand.append(y[i-time_point_week+j*24*6])\n",
    "    f1.append(last_week_demand)\n",
    "f1=np.array(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the f2\n",
    "time_label=[t[-8:-3] for t in df['DateTime']]\n",
    "time2catg=dict(zip(time_label, range(len(set(time_label)))))\n",
    "f2=[time2catg[t] for t in time_label[time_point_week:]]\n",
    "f2=np.array(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the f3, f4\n",
    "f3=[]\n",
    "f4=[]\n",
    "for i in range(time_point_week,len(df)):\n",
    "    last_two_hour = X[i-12:i].values\n",
    "    last_two_hour_demand =y[i-12:i]\n",
    "    f3.append(last_two_hour)\n",
    "    f4.append(last_two_hour_demand)\n",
    "f3=np.array(f3)\n",
    "f4=np.array(f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(array, test_size):\n",
    "    num_samples = len(array[0])\n",
    "    permutation = np.random.permutation(range(num_samples))\n",
    "    train_split,test_split = [], []\n",
    "    test_set_size = int(num_samples*test_size)\n",
    "    for arr in array:\n",
    "        train_split.append(arr[permutation][:-test_set_size])\n",
    "        test_split.append(arr[permutation][-test_set_size:])\n",
    "    return train_split, test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    normalize_res=[]\n",
    "    mu_arr=[]\n",
    "    theta_arr=[]\n",
    "    for arr in array:\n",
    "        mu, theta =np.mean(arr,axis=0),np.max(arr,axis=0)-np.min(arr,axis=0)\n",
    "        normalize_res.append((arr-mu)/theta)\n",
    "        mu_arr.append(mu)\n",
    "        theta_arr.append(theta)\n",
    "    return tuple(normalize_res), mu_arr, theta_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f1, f3, f4), mu_arr, theta_arr = normalize([f1, f3, f4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10% data as test set \n",
    "train_split, test_split = train_test_split(\n",
    "      [np.log(y[time_point_week:]).values, f1, f2, f3, f4],test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, val_split = train_test_split(\n",
    "      train_split,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"./processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y:(24678,)\n",
      "train_f1:(24678, 7)\n",
      "train_f2:(24678,)\n",
      "train_f3:(24678, 12, 4)\n",
      "train_f4:(24678, 12)\n",
      "val_y:(12154,)\n",
      "val_f1:(12154, 7)\n",
      "val_f2:(12154,)\n",
      "val_f3:(12154, 12, 4)\n",
      "val_f4:(12154, 12)\n",
      "test_y:(4092,)\n",
      "test_f1:(4092, 7)\n",
      "test_f2:(4092,)\n",
      "test_f3:(4092, 12, 4)\n",
      "test_f4:(4092, 12)\n"
     ]
    }
   ],
   "source": [
    "for name, arr in zip([\"train\",\"val\",\"test\"],[train_split, val_split, test_split]):\n",
    "    for col,i in zip(['y','f1','f2','f3','f4'],arr):\n",
    "        print(f\"{name}_{col}: {i.shape}\")\n",
    "        np.save(f\"./processed/{name}_{col}.npy\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(r2_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb3ee2cb987f0791d59217aac9c93edb0025d3eec1e91adab0e3ff51219fcd98"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('areix': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
